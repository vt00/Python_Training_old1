{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn Algorithms in Three Phases\n",
    "\n",
    "Each of Scikit-Learn's supervised learners is represented as a python class.\n",
    "For example, there is a LogisticRegression class, a DecisionTreeClassifier\n",
    "class, a RandomForestRegressor class, and so on. For simplicity, each of\n",
    "these classes uses the same application programming interface (\"API\").\n",
    "That is, to use these classes, you will invoke the same methods regardless\n",
    "of whether you are building a logistic regression or a random forest.  \n",
    "\n",
    "Using these classes typically consists of **three phases**.  \n",
    "\n",
    "1. Initialization phase\n",
    "2. Training phase\n",
    "3. Prediction phase\n",
    "\n",
    "\n",
    "## INITIALIZATION PHASE\n",
    "In the initialization phase, we create a new learner and specify any\n",
    "options we want. Let's use the example of a logistic regression.\n",
    "Remember, the LogisticRegression is a general class (like a \"blueprint\") not a\n",
    "specific object just like the word \"list\" is a class.  \n",
    "\n",
    "To create a specific list you must type:  \n",
    "   `mylist = list(1,2,3)`  \n",
    "Similarly, to create a specific logistic regressor type:  \n",
    "   `lr = LogisticRegression()`  \n",
    "   \n",
    "When we're creating our regression object, we should specify any details.\n",
    "For example, do we want the module to be regularized regression? If so how much? \n",
    "\n",
    "Let's create a `LogisticRegression` object and specify some options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import it from the relevant module\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Standard logistic regression\n",
    "lr = LogisticRegression()\n",
    "\n",
    "\n",
    "#Logistic with a penalty parameter of 0.1\n",
    "lr = LogisticRegression(C = 0.5)\n",
    "\n",
    "\n",
    "#another option is \"class_weight\". If you have two classes A & B,\n",
    "#and 90% of the observations are of class A, sometimes it can be\n",
    "#helpful for your model to give more weight to class B to give your\n",
    "#model more balance. To do this, set the parameter class_weight = \"balanced\"\n",
    "lr = LogisticRegression(C = 0.5, class_weight = \"balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression is relatively simple, so it doesn't have many options.\n",
    "Other algorithms, like random forest, are more complicated and have far\n",
    "more options to choose from. For instance you have the number of tree\n",
    "\n",
    "\n",
    "## TRAINING PHASE\n",
    "\n",
    "Assume you have some predictive data stored in a variable X and you\n",
    "have the target data stored in a variable called y. Imagine you split\n",
    "the data into a training and testing set called\n",
    "X_train, y_train and\n",
    "X_test, y_test\n",
    "\n",
    "To train any scikit-learn model, use the fit() method like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.5, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's make some simple fake X and y data\n",
    "X_train = [[1,2], [3,3], [4,5], [6,5]]\n",
    "y_train = [0, 0, 1, 1]\n",
    "\n",
    "#Train the model on the training data\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the \"fit\" method return? It doesn't really return anything.  \n",
    "Instead it changes the lr object. In this case, it stores the model's coefficients\n",
    "and intercept in attributes:  \n",
    "`lr.coef_`  \n",
    "`lr.intercept_`  \n",
    "\n",
    "These attributes will be used later on when we make predictions on our test data.  \n",
    "Let's take a look at those attributes now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09796461  0.0486127 ]]\n",
      "[-0.04638885]\n"
     ]
    }
   ],
   "source": [
    "print (lr.coef_)\n",
    "print (lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention, scikit-learn *names any learned attributes with a trailing\n",
    "underscore*.  \n",
    "Any attributes that are set during the initialization phase\n",
    "do not have a trailing underscore.  \n",
    "That's why `lr.class_weight` does not have an underscore, but `lr.coef_` does.  \n",
    "  \n",
    "Note: technically speaking, \"fit()\" returns the lr object itself. So if you\n",
    "wanted, you could type \"lr = lr.fit(X, y)\" but this is not necessary.  \n",
    "  \n",
    "That's basically everything when it comes to the fit method. There aren't a\n",
    "lot of options to specify. Most of the options have been specified\n",
    "during the initialization phase.\n",
    "\n",
    "\n",
    "\n",
    "## PREDICTION PHASE\n",
    "\n",
    "In the prediction phase, we test our model on the unseen predictive data\n",
    "and compare its output to the unseen target data.\n",
    "\n",
    "To make a prediction with any scikit-learn model, use the predict() method.\n",
    "It will return a numpy array of the model's best estimates.  \n",
    "\n",
    "Let's make some fake test data and make a prediction to see how we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1]\n",
      "[0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#Fake Data\n",
    "X_test = [[1,1], [3,2.5], [4,4.5], [5.8,4.5]]\n",
    "y_test = [0, 0, 1, 1]\n",
    "\n",
    "\n",
    "#Make a prediction\n",
    "prediction = lr.predict(X_test)\n",
    "\n",
    "#print the results so we can see\n",
    "print(prediction)\n",
    "          \n",
    "#print the \"true\" values to we can compare\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If X_test contains 5 observations, then \"prediction\" will be an array with\n",
    "5 elements. For example it might be [0,0,1,1,0].  \n",
    "\n",
    "Typically you would then compare this to y_test to see how well the model did.\n",
    "For example, y_test might be [0,0,1,1,1]. In this case, the model got 4 out of 5\n",
    "predictions correct.  \n",
    "\n",
    "Note: if this were a regression (e.g. LinearRegression) the pred would be an\n",
    "array of floats, e.g. [0.1, 1.2, 1.5, 2.0]  \n",
    "\n",
    "### Getting Probability Scores\n",
    "\n",
    "Under the hood, classifiers almost always produce a number between 0 and 1\n",
    "(in the case of binary classification anyway) that represents the probability\n",
    "that an observation belongs to class 1. It then takes any value below 0.5 and\n",
    "classifies it as \"0\" then takes any value above 0.5 and classifies it as \"1\".  \n",
    "\n",
    "Sometimes it's helpful to have this value rather than a simple 0 or 1 class.  \n",
    "In this case use the `predict_proba()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.50725906  0.49274094]\n",
      " [ 0.39186419  0.60813581]\n",
      " [ 0.3327938   0.6672062 ]\n",
      " [ 0.25092859  0.74907141]]\n"
     ]
    }
   ],
   "source": [
    "#Get probabilities of belonging to class 0 or 1\n",
    "probs = lr.predict_proba(X_test)\n",
    "\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"probs\" is a table containig the probability of each class for reach observation.\n",
    "Each class is represented by a column and each observation is represented by a row.\n",
    "For example, row 0 might have the columns [0.25, 0.75]. This means row 0 has a 25%\n",
    "chance of belonging to class 0 and a 75% chance of belonging to class 1.  \n",
    "\n",
    "If there were more than two classes (say, classes 0, 1, 2 and 3) then there would be\n",
    "additional columns in probs. Each row will sum to 1. For example it might be:  \n",
    "[[0.25, 0.25, 0.3, 0.2],  \n",
    "[0.1, 0.85, 0.03, 0.02]  \n",
    "....  \n",
    "[0.2, 0.4, 0.3, 0.1]]  \n",
    "\n",
    "\n",
    "### TL;DR \n",
    "\n",
    "Here's a quick recap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [0 1 1 1]\n",
      "\n",
      "Probabilities:\n",
      "[[ 0.50725906  0.49274094]\n",
      " [ 0.39186419  0.60813581]\n",
      " [ 0.3327938   0.6672062 ]\n",
      " [ 0.25092859  0.74907141]]\n"
     ]
    }
   ],
   "source": [
    "#Create a specific classifier, specify options\n",
    "lr = LogisticRegression(C = 0.5)\n",
    "\n",
    "#Train on the train data\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "#Predict on the test data\n",
    "prediction = lr.predict(X_test)\n",
    "\n",
    "#To get predicted probabilities rather than predicted classes do  this:\n",
    "probs = lr.predict_proba(X_test)\n",
    "\n",
    "print(\"Prediction: \", prediction)\n",
    "print (\"\\nProbabilities:\\n\", probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
